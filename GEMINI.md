# ğŸ¯ CORE CONTEXT: 1-Week CRM Data Platform
- **Goal**: Demonstrate DE skills via high-load (Spike) handling and hybrid infra setup.
- **Tech Stack**: Kafka(3-Node VM), Spark on K8s, Delta Lake, MinIO, dbt, PostgreSQL, Airflow, Prometheus/Grafana
- **Infrastructure**: 6 Mini-PCs (Proxmox) | 1 Desktop (Ubuntu) | K8s Cluster | Proxmox Host/VM Management via Ansible
- **Network**: Private (192.168.0.0/24)
- **Repositories**: `home-server-infra` (Base), `data-platform-core` (Logic), `data-generator` (Source)

# ğŸ“œ SHARED RULES

## ì»¤ë°‹ ë©”ì‹œì§€ ê·œì¹™
- **í˜•ì‹**: `type: Verb description #issue`
- **Type**: feat, fix, docs, perf, refactor, chore, test (ì†Œë¬¸ì)
- **Verb**: ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ëª…ë ¹í˜• ë™ì‚¬ (ì˜ˆ: Add, Fix, Update, Remove)
- **ì˜ˆì‹œ**: `feat: Add Spark streaming pipeline #12`, `fix: Correct Kafka consumer lag handling #15`
- **ìƒì„¸**: ì²« ì¤„ ì´í›„ ë¹ˆ ì¤„ì„ ë‘ê³  ë³¸ë¬¸ì— ë³€ê²½ ì´ìœ ì™€ ì˜í–¥ ë²”ìœ„ ì‘ì„±

## ë°ì´í„° íŒŒì´í”„ë¼ì¸ íë¦„
**Logic Flow**: `data-generator` -> `Kafka` -> `Spark` -> `Delta Lake` -> `dbt` -> `Dashboard`

## ë¬¸ì„œ ì°¸ì¡°
- í•­ìƒ `docs/ProejctRFC-#1.md` (í”„ë¡œì íŠ¸ ì •ì˜ì„œ) ì°¸ì¡°
- ì‘ì—… ì „ `docs/ai-context/` ìµœì‹  íŒŒì¼ í™•ì¸ í•„ìˆ˜
- ê¸°ìˆ  ê²°ì • ì‚¬í•­ì€ `docs/ai-context/issue-[ë²ˆí˜¸]-[ì œëª©].md` í˜•ì‹ìœ¼ë¡œ ê¸°ë¡

## ì œì•½ ì‚¬í•­
- Proxmox/K8s í•˜ì´ë¸Œë¦¬ë“œ í™˜ê²½ì˜ ìì› ì œì•½ ì¤€ìˆ˜
- Spike íŠ¸ë˜í”½ (3K TPS) ì²˜ë¦¬ ì„±ëŠ¥ ìµœì í™” ìš°ì„ ìˆœìœ„
- ê³ ê°€ìš©ì„± (High Availability) ë³´ì¥ í•„ìš”

# ğŸ“‘ CONTEXT SYNC PROTOCOL
1. Read `docs/ProejctRFC-#1.md` (í”„ë¡œì íŠ¸ ì •ì˜ì„œ) and `docs/ai-context/` first.
2. Draft technical specs before coding.
3. Log the "Tiki-Taka" discussion into `docs/ai-context/` after completing tasks.
4. ë ˆí¬ì§€í† ë¦¬ ê°„ ì˜ì¡´ì„±ì„ ê³ ë ¤í•˜ì—¬ ë³€ê²½ ì‚¬í•­ì„ ë¬¸ì„œí™”í•˜ì‹­ì‹œì˜¤.

